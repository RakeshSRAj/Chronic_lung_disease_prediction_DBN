import os
import boto3
import numpy as np
import tensorflow as tf
from PIL import Image
from io import BytesIO
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import BernoulliRBM
from imblearn.over_sampling import SMOTE
import pyswarms as ps

# Constants
BUCKET = 'your-bucket-name'
PREFIX = 'cxr-dataset'
REGION = boto3.Session().region_name
s3 = boto3.client('s3')

# 1. Load images directly from S3
def load_images_from_s3(bucket, prefix, target_size=(224, 224)):
    X, y = [], []
    paginator = s3.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)

    for page in pages:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.endswith('.jpg') or key.endswith('.png'):
                label = key.split('/')[-2]
                response = s3.get_object(Bucket=bucket, Key=key)
                img_bytes = response['Body'].read()
                img = Image.open(BytesIO(img_bytes)).convert('RGB')
                img = img.resize(target_size)
                X.append(np.array(img))
                y.append(label)
    return np.array(X), np.array(y)

X, y = load_images_from_s3(BUCKET, PREFIX)

# Encode class labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Preprocess input for MobileNetV2
X = tf.keras.applications.mobilenet_v2.preprocess_input(X)

# 2. MobileNetV2 Feature Extraction
mobilenet = tf.keras.applications.MobileNetV2(include_top=False, pooling='avg', weights='imagenet', input_shape=(224,224,3))
features = mobilenet.predict(X, batch_size=32, verbose=1)

# 3. SMOTE for balancing
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(features_scaled, y_encoded)

# 4. Split data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)

# 5. DBN model builder
def create_dbn(params):
    n_components = int(params[0])
    learning_rate = params[1]
    n_iter = int(params[2])
    
    rbm = BernoulliRBM(n_components=n_components, learning_rate=learning_rate, n_iter=n_iter, random_state=42)
    logistic = LogisticRegression(max_iter=1000)
    dbn = Pipeline([('rbm', rbm), ('logistic', logistic)])
    return dbn

# 6. PSO + BER optimizer
def objective_function(params):
    losses = []
    for p in params:
        try:
            model = create_dbn(p)
            model.fit(X_train, y_train)
            acc = model.score(X_test, y_test)
            losses.append(-acc)  # Maximize accuracy â†’ minimize negative
        except:
            losses.append(1.0)
    return np.array(losses)

# Hyperparameter bounds: [n_components, learning_rate, n_iter]
bounds = ([50, 0.001, 5], [200, 0.1, 30])
optimizer = ps.single.GlobalBestPSO(n_particles=10, dimensions=3,
                                    options={'c1': 0.5, 'c2': 0.3, 'w': 0.9},
                                    bounds=bounds)
best_cost, best_params = optimizer.optimize(objective_function, iters=10)

# 7. Train final model
final_model = create_dbn(best_params)
final_model.fit(X_train, y_train)
y_pred = final_model.predict(X_test)

# 8. Classification report
print("Classification Report for 14 Chronic Lung Diseases:")
print(classification_report(y_test, y_pred, target_names=le.inverse_transform(np.unique(y_encoded))))
