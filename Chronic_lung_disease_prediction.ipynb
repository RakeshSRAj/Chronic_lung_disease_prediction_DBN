!pip install imbalanced-learn sagemaker boto3 --quiet

import boto3
import sagemaker
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

#load data
sagemaker_session = sagemaker.Session()
bucket = 'your-s3-bucket-name'  # CHANGE THIS
prefix = 'lung-disease'
region = boto3.Session().region_name

# Download the CSV from S3
s3_uri = f's3://{bucket}/{prefix}/lung_disease.csv'
df = pd.read_csv(s3_uri)

print("Data shape:", df.shape)


#Prerocess and SMOTE
X = df.drop('target', axis=1)
y = df['target']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

print("After SMOTE:", X_resampled.shape, y_resampled.shape)

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

#DBN
class DBN(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        super(DBN, self).__init__()
        layers = []
        dims = [input_dim] + hidden_dims
        for i in range(len(dims) - 1):
            layers.append(nn.Linear(dims[i], dims[i + 1]))
            layers.append(nn.ReLU())
        layers.append(nn.Linear(dims[-1], output_dim))
        layers.append(nn.Sigmoid())
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

#PSOBER
def fitness_function(params, X_train_tensor, y_train_tensor):
    lr = float(params[0])
    h1 = int(params[1])
    h2 = int(params[2])

    model = DBN(X_train_tensor.shape[1], [h1, h2], 1)
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    for epoch in range(10):  # Light training for fitness evaluation
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        optimizer.step()

    with torch.no_grad():
        model.eval()
        preds = model(X_train_tensor).round()
        acc = accuracy_score(y_train_tensor, preds)
    return -acc  # Minimize negative accuracy

def ber_pso_optimize(X_train, y_train, n_particles=10, n_iter=10):
    dim = 3
    particles = np.random.rand(n_particles, dim)
    velocities = np.random.rand(n_particles, dim)

    personal_best = particles.copy()
    global_best = particles[0]
    global_best_score = float('inf')

    # Convert data
    X_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)

    personal_scores = [fitness_function(p, X_tensor, y_tensor) for p in particles]

    for i, score in enumerate(personal_scores):
        if score < global_best_score:
            global_best_score = score
            global_best = particles[i]

    for iter in range(n_iter):
        for i in range(n_particles):
            # BER-inspired update: move toward best solution with exploration
            r1, r2 = np.random.rand(2)
            velocities[i] = (r1 * (personal_best[i] - particles[i])) + (r2 * (global_best - particles[i]))
            particles[i] += velocities[i]

            # Clamp values
            particles[i][0] = np.clip(particles[i][0], 0.0001, 0.01)  # lr
            particles[i][1:] = np.clip(particles[i][1:], 10, 100)     # hidden dims

            score = fitness_function(particles[i], X_tensor, y_tensor)

            if score < personal_scores[i]:
                personal_scores[i] = score
                personal_best[i] = particles[i]
                if score < global_best_score:
                    global_best = particles[i]
                    global_best_score = score

        print(f"Iteration {iter}, Best score: {-global_best_score:.4f}")

    return global_best

#Train DBN with best hyperparameters
best_params = ber_pso_optimize(X_train, y_train)
lr, h1, h2 = float(best_params[0]), int(best_params[1]), int(best_params[2])

print(f"Best Hyperparameters: LR={lr:.5f}, Hidden1={h1}, Hidden2={h2}")

model = DBN(X_train.shape[1], [h1, h2], 1)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)

# Final training
for epoch in range(50):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

#evaluate model
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_pred = model(X_test_tensor).detach().numpy().round()
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

#save results
import json

results = {
    'accuracy': float(accuracy),
    'best_params': {
        'learning_rate': lr,
        'hidden1': h1,
        'hidden2': h2
    }
}

with open('results.json', 'w') as f:
    json.dump(results, f)

output_uri = sagemaker_session.upload_data('results.json', bucket=bucket, key_prefix=f'{prefix}/output')
print(f"Results uploaded to: {output_uri}")


